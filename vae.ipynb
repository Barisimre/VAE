{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "INPUT_SIZE = 11\n",
    "LAYER_SIZE = 20\n",
    "LATENT_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data from the file\n",
    "\n",
    "saved = np.load(\"testing.npy\", allow_pickle=True)\n",
    "\n",
    "# Make a trainging and testing batch\n",
    "train_data = torch.Tensor(saved[:int(len(saved)*0.9)])\n",
    "test_data = torch.Tensor(saved[int(len(saved*0.9)):])\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.l1 = nn.Linear(INPUT_SIZE, LAYER_SIZE)\n",
    "        self.l2a = nn.Linear(LAYER_SIZE, LATENT_SIZE)\n",
    "        self.l2b = nn.Linear(LAYER_SIZE, LATENT_SIZE)\n",
    "        \n",
    "        # Decoder\n",
    "        self.l3 = nn.Linear(LATENT_SIZE, LAYER_SIZE)\n",
    "        self.l4 = nn.Linear(LAYER_SIZE, INPUT_SIZE)\n",
    "    \n",
    "    # Run some data through the encoder\n",
    "    def encode(self, x):\n",
    "        out = F.relu(self.l1(x))\n",
    "        # return the mu and the sigma\n",
    "        return self.l2a(out), self.l2b(out)\n",
    "    \n",
    "    # The reparameterization trick, taken from https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "    def reparameterize(self, mu, sigma):\n",
    "        std = torch.exp(0.5*sigma)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, x):\n",
    "        out = F.relu(self.l3(x))\n",
    "        return torch.sigmoid(self.l4(out)) # sigmoid vs tanh\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: check the shape of x to be sure we have the right input\n",
    "        mu, sigma = self.encode(x)\n",
    "        z = self.reparameterize(mu, sigma)\n",
    "        # The loss function needs the mu and the sigma so just return them here\n",
    "        return self.decode(z), mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "# Taken from https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "def loss_function(recon_x, x, mu, sigma):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + sigma - mu.pow(2) - sigma.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE().to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(e_count):\n",
    "    vae.train()\n",
    "    bs = 0\n",
    "    train_loss = 0\n",
    "    for i in range(0, len(train_data), BATCH_SIZE):\n",
    "        batch = train_data[i:i+BATCH_SIZE].to(device)\n",
    "        vae.zero_grad()\n",
    "        recons, mu, sigma = vae(batch)\n",
    "        loss = loss_function(recons, batch, mu, sigma)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch number: {e_count}, loss: {train_loss/len(train_data)}\")        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 0, loss: -64.5823937151591\n",
      "Epoch number: 1, loss: -102.56528255750868\n",
      "Epoch number: 2, loss: -114.20159628634983\n",
      "Epoch number: 3, loss: -120.57638023003472\n",
      "Epoch number: 4, loss: -124.45075209418403\n",
      "Epoch number: 5, loss: -127.61899962565104\n",
      "Epoch number: 6, loss: -131.744256241862\n",
      "Epoch number: 7, loss: -135.0486755750868\n",
      "Epoch number: 8, loss: -138.61743599989148\n",
      "Epoch number: 9, loss: -141.48094783799914\n",
      "Epoch number: 10, loss: -143.74610743815103\n",
      "Epoch number: 11, loss: -145.6809415011936\n",
      "Epoch number: 12, loss: -147.9759304877387\n",
      "Epoch number: 13, loss: -150.16247517903645\n",
      "Epoch number: 14, loss: -152.8623485921224\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    train(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
